# -*- coding: utf-8 -*-
"""GPT_Neo_(1) (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DfnFJVWNTjnzE2VE3SDz6GGmfxo5oTn5
"""

import pandas as pd
from transformers import GPT2Tokenizer
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPTNeoModel
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from transformers import AdamW

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/post_processing_csv_data_v1.csv'

df = pd.read_csv(file_path)
df.head()

def split_dataset(df, split_ratio=0.8):
    """
    Splits the dataset into two portions sequentially based on the given ratio.

    Args:
    - df (pd.DataFrame): The input dataset.
    - split_ratio (float): Ratio for the first portion (default: 0.8 for an 80-20 split).

    Returns:
    - df_train (pd.DataFrame): The first portion of the dataset.
    - df_test (pd.DataFrame): The second portion of the dataset.
    """
    # Calculates the split index
    split_index = int(len(df) * split_ratio)

    # Splits the DataFrame
    df_train = df.iloc[:split_index]
    df_test = df.iloc[split_index:]

    return df_train, df_test


input_df, df_test = split_dataset(df, split_ratio=0.5)

print(f"Training set size: {len(input_df)}")
print(f"Test set size: {len(df_test)}")

# Checks for GPU availability and set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initializes the tokenizer for GPT-Neo (GPT-Neo uses GPT2Tokenizer)
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

# Sets the pad token to eos_token
tokenizer.pad_token = tokenizer.eos_token

def tokenize_data(df, max_len=128, batch_size=32):
    """
    Tokenizes the dataset for GPT-Neo and moves tensors to the GPU if available.
    """
    input_ids, attention_masks, labels = [], [], []

    # Processes the data in batches
    for start_idx in range(0, len(df), batch_size):
        batch = df[start_idx:start_idx + batch_size]

        # Prepares inputs for the tokenizer
        encoded = tokenizer(
            text=[f"{row['aspect']} {tokenizer.sep_token} {row['context']}" for _, row in batch.iterrows()],
            padding='max_length',
            truncation=True,
            max_length=max_len,
            return_tensors="pt"
        )
        # Moves tensors to the GPU
        input_ids.append(encoded['input_ids'].to(device))
        attention_masks.append(encoded['attention_mask'].to(device))
        labels.extend(batch['sentiment'].tolist())

    # Stacks the tensors for all batches
    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    labels = torch.tensor(labels, device=device)

    return input_ids, attention_masks, labels


input_ids, attention_masks, labels = tokenize_data(input_df, max_len=128)

print(f"Input IDs shape: {input_ids.shape}")
print(f"Attention Masks shape: {attention_masks.shape}")
print(f"Labels shape: {labels.shape}")

class AspectSentimentDataset(Dataset):
    def __init__(self, input_ids, attention_masks, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        return {
            'input_ids': self.input_ids[index],
            'attention_mask': self.attention_masks[index],
            'label': self.labels[index]
        }

# Tokenizes data using GPT-Neo tokenizer
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer.pad_token = tokenizer.eos_token  # Set pad token for GPT-Neo

dataset = AspectSentimentDataset(input_ids, attention_masks, labels)

# Splits into training and validation sets
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# Creates DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

class AspectSentimentClassifier(nn.Module):
    def __init__(self, gpt_neo_model_name, num_classes):
        super(AspectSentimentClassifier, self).__init__()
        self.gpt_neo = GPTNeoModel.from_pretrained(gpt_neo_model_name)
        for param in self.gpt_neo.parameters():
            param.requires_grad = False  # Freeze GPT-Neo

        self.classifier = nn.Sequential(
            nn.Linear(self.gpt_neo.config.hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )

    def forward(self, input_ids, attention_mask):
        # Passes inputs through GPT-Neo model
        outputs = self.gpt_neo(input_ids=input_ids, attention_mask=attention_mask)
        # Extract the hidden state of the last token
        cls_token = outputs.last_hidden_state[:, -1, :]  # Use the last token's hidden state
        # Passes through the classifier to generate logits
        logits = self.classifier(cls_token)
        return logits

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=5):
    model.train()
    train_accuracies = []
    val_accuracies = []
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # Training Phase
        model.train()
        total_loss = 0
        all_predictions = []
        all_labels = []

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)

            # Calculates loss
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # Calculates predictions
            predictions = outputs.argmax(dim=1).detach().cpu().numpy()
            all_predictions.extend(predictions)
            all_labels.extend(labels.detach().cpu().numpy())

        # Calculates training metrics
        train_accuracy = accuracy_score(all_labels, all_predictions)
        train_accuracies.append(train_accuracy)
        train_losses.append(total_loss / len(train_loader))

        # Validation Phase
        model.eval()
        val_predictions = []
        val_labels = []
        val_total_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)

                outputs = model(input_ids, attention_mask)

                # Calculates loss
                loss = criterion(outputs, labels)
                val_total_loss += loss.item()

                predictions = outputs.argmax(dim=1).detach().cpu().numpy()
                val_predictions.extend(predictions)
                val_labels.extend(labels.detach().cpu().numpy())

        val_accuracy = accuracy_score(val_labels, val_predictions)
        val_accuracies.append(val_accuracy)
        val_losses.append(val_total_loss / len(val_loader))

    return train_accuracies, val_accuracies, train_losses, val_losses, val_predictions, val_labels

def hyperparameter_tuning(model_class, train_loader, val_loader, criterion, epochs=5):
    best_val_accuracy = 0
    best_params = {}
    best_model = None
    best_train_accuracies = []
    best_val_accuracies = []
    best_train_losses = []
    best_val_losses = []
    best_val_predictions = []
    best_val_labels = []

    learning_rates = [1e-3, 1e-4, 1e-5]
    optimizers = [AdamW]

    for lr in learning_rates:
        for opt in optimizers:
            model = model_class().to(device)
            optimizer = opt(model.parameters(), lr=lr)

            train_accuracies, val_accuracies, train_losses, val_losses, val_predictions, val_labels = train_model(
                model, train_loader, val_loader, optimizer, criterion, epochs
            )

            if max(val_accuracies) > best_val_accuracy:
                best_val_accuracy = max(val_accuracies)
                best_params = {'optimizer': opt.__name__, 'learning_rate': lr}
                best_model = model
                best_train_accuracies = train_accuracies
                best_val_accuracies = val_accuracies
                best_train_losses = train_losses
                best_val_losses = val_losses
                best_val_predictions = val_predictions
                best_val_labels = val_labels

    precision = precision_score(best_val_labels, best_val_predictions, average='weighted')
    recall = recall_score(best_val_labels, best_val_predictions, average='weighted')
    f1 = f1_score(best_val_labels, best_val_predictions, average='weighted')

    print(f"Best Model Metrics:")
    print(f"Optimizer: {best_params['optimizer']}, Learning Rate: {best_params['learning_rate']}")
    print(f"Validation Accuracy: {best_val_accuracy:.f}")
    print(f"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1:.2f}")

    # Plots learning curves for the best model
    plt.figure(figsize=(12, 6))

    # Accuracy learning curve
    plt.subplot(1, 2, 1)
    plt.plot(range(1, epochs + 1), best_train_accuracies, label='Training Accuracy', marker='o')
    plt.plot(range(1, epochs + 1), best_val_accuracies, label='Validation Accuracy', marker='o')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Accuracy Learning Curve (Best Model)')
    plt.legend()
    plt.grid(True)

    # Loss learning curve
    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs + 1), best_train_losses, label='Training Loss', marker='o')
    plt.plot(range(1, epochs + 1), best_val_losses, label='Validation Loss', marker='o')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Loss Learning Curve (Best Model)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    return best_model

# Parameters
GPT_NEO_MODEL_NAME = "EleutherAI/gpt-neo-1.3B"
NUM_CLASSES = train_loader.dataset.num_classes
EPOCHS = 5

# Initializes hyperparameter tuning
best_model = hyperparameter_tuning(
    lambda: AspectSentimentClassifier(gpt_neo_model_name=GPT_NEO_MODEL_NAME, num_classes=NUM_CLASSES),
    train_loader,
    val_loader,
    nn.CrossEntropyLoss(),
    epochs=EPOCHS
)