# -*- coding: utf-8 -*-
"""BERT (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19FT5LtwHJzQ53aAEQF6MJftIaIdDZEFs
"""

import pandas as pd
from transformers import BertTokenizer
import torch
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import torch.nn as nn

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/post_processing_csv_data_final_v1.csv'

df = pd.read_csv(file_path)
df.head()

df['sentiment'] = df['sentiment'].map({'Positive': 1, 'Neutral': 0, 'Negative': 2})

df = df.sort_values(by='date')

df.head()

df.shape

def split_dataset(df, split_ratio=0.8):
    """
    Splits the dataset into two portions sequentially based on the given ratio.

    Args:
    - df (pd.DataFrame): The input dataset.
    - split_ratio (float): Ratio for the first portion (default: 0.8 for an 80-20 split).

    Returns:
    - df_train (pd.DataFrame): The first portion of the dataset.
    - df_test (pd.DataFrame): The second portion of the dataset.
    """
    # Calculates the split index
    split_index = int(len(df) * split_ratio)

    # Splits the DataFrame
    df_train = df.iloc[:split_index]
    df_test = df.iloc[split_index:]

    return df_train, df_test


input_df, df_test = split_dataset(df, split_ratio=0.5)

print(f"Training set size: {len(input_df)}")
print(f"Test set size: {len(df_test)}")

# Checks for GPU availability and set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Initializes the tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_data(df, max_len=128, batch_size=32):
    """
    Tokenizes the dataset for BERT and moves tensors to the GPU if available.
    """
    input_ids, attention_masks, labels = [], [], []

    # Processes the data in batches
    for start_idx in range(0, len(df), batch_size):
        batch = df[start_idx:start_idx + batch_size]

        # Prepares inputs for the tokenizer
        encoded = tokenizer(
            text=[f"[CLS] {row['aspect']} [SEP] {row['context']} [SEP]" for _, row in batch.iterrows()],
            padding='max_length',
            truncation=True,
            max_length=max_len,
            return_tensors="pt"
        )
        # Moves tensors to the GPU
        input_ids.append(encoded['input_ids'].to(device))
        attention_masks.append(encoded['attention_mask'].to(device))
        labels.extend(batch['sentiment'].tolist())

    # Stacks the tensors for all batches
    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    labels = torch.tensor(labels, device=device)

    return input_ids, attention_masks, labels


input_ids, attention_masks, labels = tokenize_data(input_df, max_len=128)

print(f"Input IDs shape: {input_ids.shape}")
print(f"Attention Masks shape: {attention_masks.shape}")
print(f"Labels shape: {labels.shape}")

from torch.utils.data import Dataset, DataLoader

class AspectSentimentDataset(Dataset):
    def __init__(self, input_ids, attention_masks, labels):
        self.input_ids = input_ids
        self.attention_masks = attention_masks
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        return {
            'input_ids': self.input_ids[index],
            'attention_mask': self.attention_masks[index],
            'label': self.labels[index]
        }

# Create the dataset
dataset = AspectSentimentDataset(input_ids, attention_masks, labels)

# Define indices for train, validation, and test sets
dataset_length = len(dataset)
train_size = int(0.6 * dataset_length)
val_size = int(0.3 * dataset_length)
test_size = dataset_length - train_size - val_size

# Sequential split
train_indices = list(range(0, train_size))
val_indices = list(range(train_size, train_size + val_size))
test_indices = list(range(train_size + val_size, dataset_length))

# Subset datasets
train_dataset = torch.utils.data.Subset(dataset, train_indices)
val_dataset = torch.utils.data.Subset(dataset, val_indices)
test_dataset = torch.utils.data.Subset(dataset, test_indices)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

from transformers import BertModel
import torch.nn as nn
import torch

class AspectSentimentClassifier(nn.Module):
    def __init__(self, bert_model_name, num_classes):
        super(AspectSentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(bert_model_name)
        for param in self.bert.parameters():
            param.requires_grad = False  # Freeze BERT
        self.classifier = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, 16),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(16, num_classes)
        )

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = outputs.last_hidden_state[:, 0, :]  # CLS token
        logits = self.classifier(cls_token)
        return logits

# Initialize the model
bert_model_name = 'bert-base-uncased'  # Replace with the desired BERT model
num_classes = 3  # Replace with the actual number of classes
model = AspectSentimentClassifier(bert_model_name, num_classes).to(device)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=5):
    train_accuracies = []
    val_accuracies = []
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # Training Phase
        model.train()
        total_loss = 0
        all_predictions = []
        all_labels = []

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask)

            # Calculates loss
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # Calculates predictions
            predictions = outputs.argmax(dim=1).detach().cpu().numpy()
            all_predictions.extend(predictions)
            all_labels.extend(labels.detach().cpu().numpy())

        # Calculates training metrics
        train_accuracy = accuracy_score(all_labels, all_predictions)
        train_accuracies.append(train_accuracy)
        train_losses.append(total_loss / len(train_loader))

        # Validation Phase
        model.eval()
        val_predictions = []
        val_labels = []
        val_total_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['label'].to(device)

                outputs = model(input_ids, attention_mask)

                loss = criterion(outputs, labels)
                val_total_loss += loss.item()

                predictions = outputs.argmax(dim=1).detach().cpu().numpy()
                val_predictions.extend(predictions)
                val_labels.extend(labels.detach().cpu().numpy())

        val_accuracy = accuracy_score(val_labels, val_predictions)
        val_accuracies.append(val_accuracy)
        val_losses.append(val_total_loss / len(val_loader))

    return train_accuracies, val_accuracies, train_losses, val_losses

def hyperparameter_tuning(model_class, train_loader, val_loader, criterion, epochs=5):
    best_val_accuracy = 0
    best_params = {}
    best_model = None
    best_train_accuracies = []
    best_val_accuracies = []
    best_train_losses = []
    best_val_losses = []

    learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]
    optimizers = ['Adam', 'Adagrad']

    for lr in learning_rates:
        for opt in optimizers:
            model = model_class().to(device)
            if opt == 'Adam':
                optimizer = torch.optim.Adam(model.parameters(), lr=lr)
            elif opt == 'Adagrad':
                optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)

            train_accuracies, val_accuracies, train_losses, val_losses = train_model(
                model, train_loader, val_loader, optimizer, criterion, epochs
            )

            if max(val_accuracies) > best_val_accuracy:
                best_val_accuracy = max(val_accuracies)
                best_params = {'optimizer': opt, 'learning_rate': lr}
                best_model = model
                best_train_accuracies = train_accuracies
                best_val_accuracies = val_accuracies
                best_train_losses = train_losses
                best_val_losses = val_losses

    print(f"Best Model Metrics:")
    print(f"Optimizer: {best_params['optimizer']}, Learning Rate: {best_params['learning_rate']}")
    print(f"Best Validation Accuracy: {best_val_accuracy:.4f}")

    # Plots learning curves for the best model
    plt.figure(figsize=(12, 6))

    # Accuracy learning curve
    plt.subplot(1, 2, 1)
    plt.plot(range(1, epochs + 1), best_train_accuracies, label='Train Accuracy', marker='o')
    plt.plot(range(1, epochs + 1), best_val_accuracies, label='Validation Accuracy', marker='o')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Accuracy Learning Curve (Best Model)')
    plt.legend()
    plt.grid(True)

    # Loss learning curve
    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs + 1), best_train_losses, label='Train Loss', marker='o')
    plt.plot(range(1, epochs + 1), best_val_losses, label='Validation Loss', marker='o')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Loss Learning Curve (Best Model)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    return best_model

model = AspectSentimentClassifier('bert', num_classes=3).to(device)
best_model = hyperparameter_tuning(MyModelClass, train_loader, val_loader, nn.CrossEntropyLoss(), epochs=5)